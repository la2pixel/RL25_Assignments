\documentclass[11pt]{article}

\usepackage{amsmath,amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}

% Define the invert color command
\newcommand{\invertcolor}{%
    \pagecolor{black}% Set background to black
    \color{white}% Set text color to white
}
% \invertcolor

\title{Reinforcement Learning HW 7}
\author{
Mert Bilgin (7034879) \\
\url{mert.bilgin@student.uni-tuebingen.de}
\and
Lalitha Sivakumar (6300674) \\
\url{lalitha.sivakumar@student.uni-tuebingen.de}
\and
Kevin Van Le (7314700) \\
\url{kevin-van.le@student.uni-tuebingen.de}
}
\date{\today}

\begin{document}
\maketitle

\section{Score Function for Gaussian Policy}

\subsection*{(a)} 

To compute the policy gradient, we use the log-likelihood trick. This relates the gradient of the policy to the gradient of its logarithm:

\begin{equation}
\nabla_\theta \pi_\theta(s,a) = \pi_\theta(s,a) \cdot \frac{\nabla_\theta \pi_\theta(s,a)}{\pi_\theta(s,a)} = \pi_\theta(s,a) \cdot \nabla_\theta \log \pi_\theta(s,a)
\end{equation}


We now compute the gradient of the policy with respect to $\theta$. Taking the gradient of the Gaussian policy:

\begin{align}
\nabla_\theta \pi_\theta(s,a) &= \nabla_\theta p(a|s,\theta) \nonumber \\
&= \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(a-\mu(s,\theta))^2}{2\sigma^2}\right) \cdot \left(-\frac{(a-\mu(s,\theta))}{\sigma^2}\right) \cdot \nabla_\theta \mu(s,\theta)
\end{align}

Since $\nabla_\theta \mu(s,\theta) = \nabla_\theta (\phi(s)^\top \theta) = \phi(s)$, we can simplify:

\begin{equation}
\nabla_\theta \pi_\theta(s,a) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(a-\mu(s,\theta))^2}{2\sigma^2}\right) \frac{(\mu(s,\theta) - a)}{\sigma^2} \cdot \phi(s)
\end{equation}

Finally, we obtain the gradient of the log policy by dividing the gradient by the policy itself:

\begin{equation}
\nabla_\theta \log \pi_\theta(s,a) = \frac{\nabla_\theta \pi_\theta(s,a)}{\pi_\theta(s,a)} = \frac{\phi(s)^\top \theta - a}{\sigma^2} \phi(s)
\end{equation}

\section{Deep Q-Learning (Q-Networks)}
\subsection*{(a)} see Gym-DQN.ipynb
\subsection*{(b)} see Gym-DQN.ipynb

\subsection*{(c)}
For the Pendulum task we compare three settings: target update every step (Interval=1), no target network, and target update every 20 steps (Interval=20).

\noindent\includegraphics[width=\textwidth]{pendulum_comparison.png}

\noindent The printed results are:

\begin{verbatim}
	Pendulum Reward Summary:
	Interval=1:  Train (last ep)=-1032.6, Test=-147.7+/-83.6
	No Target:   Train (last ep)=-130.9, Test=-187.5+/-95.9
	Interval=20: Train (last ep)=-263.1, Test=-643.2+/-1150.5
\end{verbatim}

\noindent The rewards are very noisy and differ a lot between runs. The train rewards indicate the agent with an interval of 20 yields the highest average rewards, but during the evaluation the performance drops a lot. This indicates overfitting.

\subsection*{(d)} see Gym-DQN.ipynb

\subsection*{(e)}
For the CartPole task we use the same three settings.

\noindent\includegraphics[width=\textwidth]{cartpole_comparison.png}

\noindent The printed results are:

\begin{verbatim}
	Cartpole Reward Summary:
	Interval=1:  Train (last ep)=176.0, Test=469.8+/-43.6
	No Target:   Train (last ep)=115.0, Test=268.0+/-56.4
	Interval=20: Train (last ep)=270.0, Test=243.9+/-22.5
\end{verbatim}

\noindent Here all agents learn to solve the task, but the agent with interval=1 gives the highest test reward on average even though again the agent with interval=20 has the highest average train reward, most likely due to overfitting.

\end{document}

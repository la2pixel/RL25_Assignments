\documentclass[11pt]{article}

\usepackage{amsmath,amssymb}
\usepackage{float}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}

% Define the invert color command
\newcommand{\invertcolor}{%
    \pagecolor{black}% Set background to black
    \color{white}% Set text color to white
}
% \invertcolor

\title{Reinforcement Learning HW 3}
\author{
Mert Bilgin (7034879) \\
\url{mert.bilgin@student.uni-tuebingen.de}
\and
Lalitha Sivakumar (6300674) \\
\url{lalitha.sivakumar@student.uni-tuebingen.de}
\and
Kevin Van Le (7314700) \\
\url{kevin-van.le@student.uni-tuebingen.de}
}
\date{\today}

\begin{document}
\maketitle

\section{Recap}
\subsection*{a)} A Markov reward process (MRP) is just a Markov chain with rewards and a discount factor. Instead of moving between states, each transition gets some reward and we care about the long-term expected return from each state. Formally, an MRP is defined as a tuple $(\mathcal{S}, P, R, \gamma)$ where:
\begin{itemize}
    \item $\mathcal{S}$ is a finite set of states,
    \item $P(S_{t+1} \mid S_t)$ is the state transition probability matrix,
    \item $R(s)$ is the expected reward on transition,
    \item $\gamma \in [0,1]$ is the discount factor.
\end{itemize}

\subsection*{b)} A MDP could be reduced to MRP when the decision-making part is removed.
This happens when we fix a policy $\pi(a \mid S_t)$ that determines which action to take in each state. 

\subsection*{c)} MRPs can be solved in closed form as they only have states, transitions, and rewards which we can describe as a system of linear equations. However in MDPs include actions, so the agent must choose among different options that affect future states and rewards. We have a max operation in the Bellman optimality equation, making it non-linear. Therefore, it can't be solved in closed form and require iterative methods for optimal solution.

\section{TD($\lambda$)}
\subsection{a)}
\begin{align*}
G_t^{(n)} &= R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1} R_{t+n} + \gamma^{n} V(S_{t+n}) \\
G_{t+1}^{(n-1)} &= R_{t+2} + \gamma R_{t+3} + \dots + \gamma^{n-2} R_{t+n} + \gamma^{n-1} V(S_{t+n}) \\
G_t^{(n)} &= R_{t+1} + \gamma \big( R_{t+2} + \gamma R_{t+3} + \dots + \gamma^{n-2} R_{t+n} + \gamma^{n-1} V(S_{t+n}) \big) \\
&= R_{t+1} + \gamma G_{t+1}^{(n-1)}
\end{align*}
\subsection*{b)}
\begin{align*}
G_t^{\lambda} &= (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_t^{(n)} \\[5pt]
\text{Using part (a), } \quad
G_t^{\lambda} &= (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} \big( R_{t+1} + \gamma G_{t+1}^{(n-1)} \big) \\[5pt]
&= R_{t+1} (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} + \gamma (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_{t+1}^{(n-1)}
\end{align*}

By the geometric series,
\[
\sum_{n=1}^{\infty} \lambda^{n-1} = \frac{1}{1 - \lambda}
\]

\begin{align*}
G_t^{\lambda} &= R_{t+1} + \gamma (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_{t+1}^{(n-1)} \\
&= R_{t+1} + \gamma (1 - \lambda) \sum_{n=0}^{\infty} \lambda^{n} G_{t+1}^{(n)} \\[5pt]
&= R_{t+1} + \gamma \Big[ (1 - \lambda) V(S_{t+1}) + \lambda G_{t+1}^{\lambda} \Big] \\
\end{align*}
\[
\boxed{G_t^{\lambda} = R_{t+1} + \gamma \Big[ (1 - \lambda) V(S_{t+1}) + \lambda G_{t+1}^{\lambda} \Big]}
\]

\section{Policy Iteration}

\subsection*{(b)}

With noise \(n = 0.2\) or \(n = 0.5\): \textbf{2 policy iterations} until the start state becomes non-zero.
\newline
Without noise \(n = 0\): \textbf{8 policy iterations} until the start state becomes non-zero.

\subsection*{(c)}

\begin{center}
\begin{tabular}{| c | c |}
\hline
\textbf{Noise Level} & \textbf{Iterations to Converge} \\
\hline
\(n = 0\) & 8 \\
\hline
\(n = 0.2\) & 2 \\
\hline
\(n = 0.5\) & 2 \\
\hline
\end{tabular}
\end{center}

\subsubsection*{Explanation for \(n=0\)}

The reason it is 8 policy iterations and not 12 policy iterations from start to finish is because
at the beginning the policy is initialized with taking the action going north for all states. For some states this is
already the optimal action, hence for a state $s_t$ where the optimal action is going north and the next state $s_{t+1}$ have
a non-zero value then state $s_t$ will also be updated with a non-zero value given that the discount is greater than 0. This value
will propagate backwards to the subsequent states as long as the policy already takes the optimal action in a given state
where the next state has a non-zero value.

\subsubsection*{Explanation for \(n>0\)}
It converges even faster with noise, because due to noise all actions will be taken with a non-zero probability and thus the optimal action is included which leads to multiple non-zero value updates and therefore faster convergence.

\subsection*{(d)}

\subsubsection*{Advantages of Policy Iteration:}

\begin{enumerate}
    \item converges faster as can be seen in 3c. Value Iteration only updates one subsequent state, i.e. it will take 12 iterations in the GridMaze for the start state to update with a non-zero value.
    \item due to faster convergence, it also needs less compute
\end{enumerate}

\subsubsection*{Disadvantages of Policy Iteration:}

\begin{enumerate}
    \item if the environment is really small, i.e. number of Value Iteration == number of Policy Iteration, then Value Iteration could be cheaper to compute as a single iteration in Policy Iteration is more expensive than in Value Iteration
    \item Policy Iteration is more complex, as not only the value function is tracked but also the policy
\end{enumerate}

\end{document}

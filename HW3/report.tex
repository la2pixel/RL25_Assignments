\documentclass[11pt]{article}

\usepackage{amsmath,amssymb}
\usepackage{float}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}

% Define the invert color command
\newcommand{\invertcolor}{%
    \pagecolor{black}% Set background to black
    \color{white}% Set text color to white
}
% \invertcolor

\title{Reinforcement Learning HW 3}
\author{
Mert Bilgin (7034879) \\
\url{mert.bilgin@student.uni-tuebingen.de}
\and
Lalitha Sivakumar (6300674) \\
\url{lalitha.sivakumar@student.uni-tuebingen.de}
\and
Kevin Van Le (7314700) \\
\url{kevin-van.le@student.uni-tuebingen.de}
}
\date{\today}

\begin{document}
\maketitle

\section{Recap}

\subsection*{a)}
\subsection*{b)}
\subsection*{c)}


\section{TD($\lambda$)}
\subsection{a)}
\begin{align*}
G_t^{(n)} &= R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1} R_{t+n} + \gamma^{n} V(S_{t+n}) \\
G_{t+1}^{(n-1)} &= R_{t+2} + \gamma R_{t+3} + \dots + \gamma^{n-2} R_{t+n} + \gamma^{n-1} V(S_{t+n}) \\
G_t^{(n)} &= R_{t+1} + \gamma \big( R_{t+2} + \gamma R_{t+3} + \dots + \gamma^{n-2} R_{t+n} + \gamma^{n-1} V(S_{t+n}) \big) \\
&= R_{t+1} + \gamma G_{t+1}^{(n-1)}
\end{align*}
\subsection*{b)}
\begin{align*}
G_t^{\lambda} &= (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_t^{(n)} \\[5pt]
\text{Using part (a), } \quad
G_t^{\lambda} &= (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} \big( R_{t+1} + \gamma G_{t+1}^{(n-1)} \big) \\[5pt]
&= R_{t+1} (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} + \gamma (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_{t+1}^{(n-1)}
\end{align*}

By the geometric series,
\[
\sum_{n=1}^{\infty} \lambda^{n-1} = \frac{1}{1 - \lambda}
\]

\begin{align*}
G_t^{\lambda} &= R_{t+1} + \gamma (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_{t+1}^{(n-1)} \\
&= R_{t+1} + \gamma (1 - \lambda) \sum_{n=0}^{\infty} \lambda^{n} G_{t+1}^{(n)} \\[5pt]
&= R_{t+1} + \gamma \Big[ (1 - \lambda) V(S_{t+1}) + \lambda G_{t+1}^{\lambda} \Big] \\
\end{align*}
\[
\boxed{G_t^{\lambda} = R_{t+1} + \gamma \Big[ (1 - \lambda) V(S_{t+1}) + \lambda G_{t+1}^{\lambda} \Big]}
\]

\section{Policy Iteration}

\subsection*{(b)}

With noise \(n = 0.2\) or \(n = 0.5\): \textbf{2 policy iterations} until the start state becomes non-zero.
\newline
Without noise \(n = 0\): \textbf{8 policy iterations} until the start state becomes non-zero.

\subsection*{(c)}

\begin{center}
\begin{tabular}{| c | c |}
\hline
\textbf{Noise Level} & \textbf{Iterations to Converge} \\
\hline
\(n = 0\) & 8 \\
\hline
\(n = 0.2\) & 2 \\
\hline
\(n = 0.5\) & 2 \\
\hline
\end{tabular}
\end{center}

\subsubsection*{Explanation for \(n=0\)}

The reason it is 8 policy iterations and not 12 policy iterations from start to finish is because
at the beginning the policy is initialized with taking the action going north for all states. For some states this is
already the optimal action, hence for a state $s_t$ where the optimal action is going north and the next state $s_{t+1}$ have
a non-zero value then state $s_t$ will also be updated with a non-zero value given that the discount is greater than 0. This value
will propagate backwards to the subsequent states as long as the policy already takes the optimal action in a given state
where the next state has a non-zero value.

\subsubsection*{Explanation for \(n>0\)}
It converges even faster with noise, because due to noise all actions will be taken with a non-zero probability and thus the optimal action is included which leads to multiple non-zero value updates and therefore faster convergence.

\subsection*{(d)}

\subsubsection*{Advantages of Policy Iteration:}

\begin{enumerate}
    \item converges faster as can be seen in 3c. Value Iteration only updates one subsequent state, i.e. it will take 12 iterations in the GridMaze for the start state to update with a non-zero value.
    \item due to faster convergence, it also needs less compute
\end{enumerate}

\subsubsection*{Disadvantages of Policy Iteration:}

\begin{enumerate}
    \item if the environment is really small, i.e. number of Value Iteration == number of Policy Iteration, then Value Iteration could be cheaper to compute as a single iteration in Policy Iteration is more expensive than in Value Iteration
    \item Policy Iteration is more complex, as not only the value function is tracked but also the policy
\end{enumerate}

\end{document}

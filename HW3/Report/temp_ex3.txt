3b)
It takes 2 policy iterations until the start state becomes non-zero. Switching noise to 0 leads leads to needing
8 policy iterations until the start state become non-zero.

3c)
n=0: 8 policy iterations
n=0.2: 2 policy iterations
n=0.5: 2 policy iterations

Note: For n=0 the reason it is 8 policy iterations and not 12 policy iterations from start to finish is because
at the beginning the policy is initialized with taking the action going north for all states. For some states this is
already the optimal action, hence for a state s_t where the optimal action is going north and the next state s_t+1 have
a non-zero value then state s_t will also be updated with a non-zero value given that the discount is > 0. This value
will propagate backwards to the subsequent states as long as the policy already takes the optimal action in a given state
where the next state has a non-zero value.

It converges even faster with noise, because due to noise all actions will be taken with a non-zero probability and thus
the optimal action is included which leads to multiple non-zero value updates and therefore faster convergence.

3d)
Pros of Policy Iteration:
- converges faster as can be seen in 3c. Value Iteration only updates one subsequent state, i.e. it will take 12
iterations in the GridMaze for the start state to update with a non-zero value.
- due to faster convergence, it also needs less compute

Cons of Policy Iteration:
- if the environment is really small, i.e. number of Value Iteration == number of Policy Iteration, then Value Iteration
could be cheaper to compute as a single iteration in Policy Iteration is more expensive than in Value Iteration
- Policy Iteration is more complex, as not only the value function is tracked but also the policy
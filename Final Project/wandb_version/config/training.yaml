# Training config: coordinator and workers use this. .env only for WANDB_API_KEY.
# Set project, entity, and all run settings here; coordinator and workers must use the same config.
#
# Algo: td3 | sac
# Reward modes: default | attack | defense | proven

project: hockey-training-phase3
entity: meloneneis   # required: your wandb username (same on all machines)

coordinator:
  max_rounds: 1
  poll_interval: 60
  timeout_hours: 24
  assignment_timeout_hours: 7   # free pool key if worker assigned but not finished for this long (crash recovery)

worker:
  poll_interval: 30

# dedicated_specs: true  = all workers must run with --algo and --reward_mode (one spec per worker).
# dedicated_specs: false = coordinator assigns a spec from the specs list to each worker automatically.
dedicated_specs: True

# Pool keys for this run (list). Format: {algo}-{reward_mode} (e.g. sac-attack, td3-defense).
# Coordinator uses this list; each worker gets one key per round.
# When dedicated_specs: true, run one worker per key, each with --algo X --reward_mode Y for that key.
specs:
  - td3-default
  - td3-attack
  - td3-defense
  - sac-default
  - sac-attack
  - sac-defense

# Global training params (apply to all specs)
training:
  total_timesteps: 8000000
  num_envs: 12
  batch_size: 256
  buffer_size: 1000000
  learning_starts: 10000
  update_ratio: 0.25
  log_freq: 20000
  eval_freq: 100000
  device: null
  save_dir: checkpoints
  model_dir: final_training_opponents
  builtin_opponents: [
    weak,
    strong,
    sac-attack-r4,
    td3-attack-r5,
    td3-defense-r3,
    sac-attack-r3,
  ]   # training opponents: weak, strong, and/or .pth (under model_dir). Repeat an entry to use it more often (e.g. [weak, strong, weak] = 2x weak).
  evaluation_opponents: [
    sac_hockey_best_attack11.pth,
    sac_hockey_best_attack12.pth,
    td3_defense_r3.pth,
    sac_attack_r3.pth,
    td3_default_r2.pth,
    td3_proven_r3.pth,
    sac_proven_r3.pth,
    sac_hockey_best_defensive3.pth,
    sac_hockey_best_proven1.pth,
    sac_hockey_best_proven3.pth
  ]
  # TD3 defaults
  policy_lr: 3.0e-4
  critic_lr: 3.0e-4
  gamma: 0.99
  polyak: 0.995
  act_noise_std: 0.1
  policy_noise: 0.2
  noise_clip: 0.5
  policy_delay: 2
  # Network size (TD3 and SAC): hidden_size used as [hidden_size, hidden_size] for both algos
  hidden_size: 512
  # TD3 regularization and decay (dropout 0 = off; weight_decay 0 = off)
  dropout: 0.0
  weight_decay: 0.0
  # TD3 improvement bundle: when true, enables dropout + weight_decay + linear decay of policy_noise, dropout, weight_decay (schedule from total_timesteps)
  improvement: True
  # SAC defaults (only used if algo: sac)
  actor_learning_rate: 3.0e-4
  critic_learning_rate: 3.0e-4
  alpha_learning_rate: 3.0e-4
  tau: 0.005
  alpha: 0.2
  pink_noise: True
  noise_beta: 1.0

# Training config: coordinator and workers use this. .env only for WANDB_API_KEY.
# Set project, entity, and all run settings here; coordinator and workers must use the same config.
#
# Algo: td3 | sac
# Reward modes: default | attack | defense | proven

project: hockey-training
entity: meloneneis   # required: your wandb username (same on all machines)
builtin_opponents: [weak, strong]

coordinator:
  max_rounds: 100
  poll_interval: 30
  timeout_hours: 24
  assignment_timeout_hours: 2   # free pool key if worker assigned but not finished for this long (crash recovery)

worker:
  poll_interval: 30

# Global training params (apply to all specs unless overridden per-spec)
training:
  total_timesteps: 4000000
  num_envs: 10
  batch_size: 256
  buffer_size: 1000000
  learning_starts: 10000
  update_ratio: 0.25
  log_freq: 5000
  eval_freq: 20000
  device: null
  save_dir: checkpoints
  eval_1v1_games: 100
  # TD3 defaults
  policy_lr: 3.0e-4
  critic_lr: 3.0e-4
  gamma: 0.99
  polyak: 0.995
  act_noise_std: 0.1
  policy_noise: 0.2
  noise_clip: 0.5
  policy_delay: 2
  # SAC defaults (only used if algo: sac)
  actor_learning_rate: 3.0e-4
  critic_learning_rate: 3.0e-4
  alpha_learning_rate: 3.0e-4
  tau: 0.005
  alpha: 0.2
  hidden_size: 512
  pink_noise: false
  noise_beta: 1.0

# Per-spec (pool key): algo + reward_mode required; other keys override "training" for that spec.
# Pool key format: {algo}-{reward_mode} (e.g. td3-default, sac-attack). Add or remove specs as needed.
specs:
  td3-default:
    algo: td3
    reward_mode: default
  td3-attack:
    algo: td3
    reward_mode: attack
  td3-proven:
    algo: td3
    reward_mode: proven
  td3-defense:
    algo: td3
    reward_mode: defense
  sac-default:
    algo: sac
    reward_mode: default
  sac-attack:
    algo: sac
    reward_mode: attack
  sac-proven:
    algo: sac
    reward_mode: proven
  sac-defense:
    algo: sac
    reward_mode: defense

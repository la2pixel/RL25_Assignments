# Training config: coordinator and workers use this. .env only for WANDB_API_KEY.
# Set project, entity, and all run settings here; coordinator and workers must use the same config.
#
# Algo: td3 | sac
# Reward modes: default | attack | defense | proven

project: hockey-training
entity: meloneneis   # required: your wandb username (same on all machines)
builtin_opponents: [weak, strong]

coordinator:
  max_rounds: 100
  poll_interval: 60
  timeout_hours: 24
  assignment_timeout_hours: 4   # free pool key if worker assigned but not finished for this long (crash recovery)

worker:
  poll_interval: 30

# dedicated_specs: true  = all workers must run with --algo and --reward_mode (one spec per worker).
# dedicated_specs: false = coordinator assigns a spec from the specs list to each worker automatically.
dedicated_specs: True

# Pool keys for this run (list). Format: {algo}-{reward_mode} (e.g. sac-attack, td3-defense).
# Coordinator uses this list; each worker gets one key per round.
# When dedicated_specs: true, run one worker per key, each with --algo X --reward_mode Y for that key.
specs:
  - td3-default
  - td3-attack
  - td3-proven
  - td3-defense
  - sac-default
  - sac-attack
  - sac-proven
  - sac-defense

# Global training params (apply to all specs)
training:
  total_timesteps: 4000000
  num_envs: 10
  batch_size: 256
  buffer_size: 1000000
  learning_starts: 10000
  update_ratio: 0.25
  log_freq: 5000
  eval_freq: 20000
  device: null
  save_dir: checkpoints
  eval_1v1_games: 100
  # TD3 defaults
  policy_lr: 3.0e-4
  critic_lr: 3.0e-4
  gamma: 0.99
  polyak: 0.995
  act_noise_std: 0.1
  policy_noise: 0.2
  noise_clip: 0.5
  policy_delay: 2
  # Network size (TD3 and SAC)
  hidden_size: 512
  # SAC defaults (only used if algo: sac)
  actor_learning_rate: 3.0e-4
  critic_learning_rate: 3.0e-4
  alpha_learning_rate: 3.0e-4
  tau: 0.005
  alpha: 0.2
  pink_noise: false
  noise_beta: 1.0
